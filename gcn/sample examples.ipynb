{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from gcn.utils import *\n",
    "from gcn.models import GCN, MLP\n",
    "\n",
    "# Set random seed\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "# Load data\n",
    "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data('citeseer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = preprocess_features(features)\n",
    "support = [preprocess_adj(adj)] # Graph Laplacian from adjacency matrix , in a compressed form through sparse_to_tuple function\n",
    "num_supports = 1\n",
    "model_func = GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define placeholders\n",
    "placeholders = {\n",
    "    'support': [tf.sparse_placeholder(tf.float32) for _ in range(num_supports)],\n",
    "    'features': tf.sparse_placeholder(tf.float32, shape=tf.constant(features[2], dtype=tf.int64)),\n",
    "    'labels': tf.placeholder(tf.float32, shape=(None, y_train.shape[1])),\n",
    "    'labels_mask': tf.placeholder(tf.int32),\n",
    "    'dropout': tf.placeholder_with_default(0., shape=()),\n",
    "    'num_features_nonzero': tf.placeholder(tf.int32)  # helper variable for sparse dropout\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gcn.layers import *\n",
    "from gcn.metrics import *\n",
    "\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "\n",
    "class Model(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        allowed_kwargs = {'name', 'logging'}\n",
    "        for kwarg in kwargs.keys():\n",
    "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
    "        name = kwargs.get('name')\n",
    "        if not name:\n",
    "            name = self.__class__.__name__.lower()\n",
    "        self.name = name\n",
    "\n",
    "        logging = kwargs.get('logging', False)\n",
    "        self.logging = logging\n",
    "\n",
    "        self.vars = {}\n",
    "        self.placeholders = {}\n",
    "\n",
    "        self.layers = []\n",
    "        self.activations = []\n",
    "\n",
    "        self.inputs = None\n",
    "        self.outputs = None\n",
    "\n",
    "        self.loss = 0\n",
    "        self.accuracy = 0\n",
    "        self.optimizer = None\n",
    "        self.opt_op = None\n",
    "\n",
    "    def _build(self):\n",
    "        raise NotImplementedError\n",
    "        # define it in the child class as needed \n",
    "\n",
    "    def build(self):\n",
    "        \"\"\" Wrapper for _build() \"\"\"\n",
    "        with tf.variable_scope(self.name):\n",
    "            self._build() # first call the child class' build\n",
    "\n",
    "        # Build sequential layer model\n",
    "        self.activations.append(self.inputs)\n",
    "        for layer in self.layers:\n",
    "            hidden = layer(self.activations[-1])\n",
    "            self.activations.append(hidden)\n",
    "        self.outputs = self.activations[-1]\n",
    "\n",
    "        # Store model variables for easy access\n",
    "        variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n",
    "        self.vars = {var.name: var for var in variables}\n",
    "\n",
    "        # Build metrics\n",
    "        self._loss()\n",
    "        self._accuracy()\n",
    "\n",
    "        self.opt_op = self.optimizer.minimize(self.loss)\n",
    "\n",
    "    def predict(self):\n",
    "        pass\n",
    "\n",
    "    def _loss(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _accuracy(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def save(self, sess=None):\n",
    "        if not sess:\n",
    "            raise AttributeError(\"TensorFlow session not provided.\")\n",
    "        saver = tf.train.Saver(self.vars)\n",
    "        save_path = saver.save(sess, \"tmp/%s.ckpt\" % self.name)\n",
    "        print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "    def load(self, sess=None):\n",
    "        if not sess:\n",
    "            raise AttributeError(\"TensorFlow session not provided.\")\n",
    "        saver = tf.train.Saver(self.vars)\n",
    "        save_path = \"tmp/%s.ckpt\" % self.name\n",
    "        saver.restore(sess, save_path)\n",
    "        print(\"Model restored from file: %s\" % save_path)\n",
    "\n",
    "\n",
    "class GCN(Model):\n",
    "    def __init__(self, placeholders, input_dim, **kwargs):\n",
    "        super(GCN, self).__init__(**kwargs)\n",
    "        self.learning_rate = 0.01\n",
    "        self.hidden1 = 16\n",
    "        self.weight_decay = 5e-4\n",
    "        self.inputs = placeholders['features']\n",
    "        self.input_dim = input_dim\n",
    "        # self.input_dim = self.inputs.get_shape().as_list()[1]  # To be supported in future Tensorflow versions\n",
    "        self.output_dim = placeholders['labels'].get_shape().as_list()[1]\n",
    "        self.placeholders = placeholders\n",
    "\n",
    "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "\n",
    "        self.build() # from Model \n",
    "\n",
    "    def _loss(self):\n",
    "        # Weight decay loss\n",
    "        for var in self.layers[0].vars.values():\n",
    "            self.loss += self.weight_decay * tf.nn.l2_loss(var)\n",
    "\n",
    "        # Cross entropy error\n",
    "        self.loss += masked_softmax_cross_entropy(self.outputs, self.placeholders['labels'],\n",
    "                                                  self.placeholders['labels_mask'])\n",
    "\n",
    "    def _accuracy(self):\n",
    "        self.accuracy = masked_accuracy(self.outputs, self.placeholders['labels'],\n",
    "                                        self.placeholders['labels_mask'])\n",
    "\n",
    "    def _build(self):\n",
    "\n",
    "        self.layers.append(GraphConvolution(input_dim=self.input_dim,\n",
    "                                            output_dim=self.hidden1,\n",
    "                                            placeholders=self.placeholders,\n",
    "                                            act=tf.nn.relu,\n",
    "                                            dropout=True,\n",
    "                                            sparse_inputs=True,\n",
    "                                            logging=self.logging))\n",
    "\n",
    "        self.layers.append(GraphConvolution(input_dim=self.hidden1,\n",
    "                                            output_dim=self.output_dim,\n",
    "                                            placeholders=self.placeholders,\n",
    "                                            act=lambda x: x,\n",
    "                                            dropout=True,\n",
    "                                            logging=self.logging))\n",
    "\n",
    "    def predict(self):\n",
    "        return tf.nn.softmax(self.outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From build/bdist.linux-x86_64/egg/gcn/metrics.py:6: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model = GCN(placeholders, input_dim=features[2][1], logging=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize session\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define model evaluation function\n",
    "def evaluate(features, support, labels, mask, placeholders):\n",
    "    t_test = time.time()\n",
    "    feed_dict_val = construct_feed_dict(features, support, labels, mask, placeholders)\n",
    "    outs_val = sess.run([model.loss, model.accuracy], feed_dict=feed_dict_val)\n",
    "    return outs_val[0], outs_val[1], (time.time() - t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Init variables\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "cost_val = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss= 1.79988 train_acc= 0.12500 val_loss= 1.79539 val_acc= 0.33400 time= 0.04680\n",
      "Epoch: 0002 train_loss= 1.79321 train_acc= 0.41667 val_loss= 1.79180 val_acc= 0.47000 time= 0.02956\n",
      "Epoch: 0003 train_loss= 1.78746 train_acc= 0.55833 val_loss= 1.78886 val_acc= 0.50800 time= 0.02776\n",
      "Epoch: 0004 train_loss= 1.78179 train_acc= 0.68333 val_loss= 1.78654 val_acc= 0.50800 time= 0.02661\n",
      "Epoch: 0005 train_loss= 1.77314 train_acc= 0.73333 val_loss= 1.78472 val_acc= 0.51400 time= 0.02699\n",
      "Epoch: 0006 train_loss= 1.76781 train_acc= 0.80833 val_loss= 1.78308 val_acc= 0.49800 time= 0.02725\n",
      "Epoch: 0007 train_loss= 1.76107 train_acc= 0.80000 val_loss= 1.78149 val_acc= 0.49600 time= 0.02664\n",
      "Epoch: 0008 train_loss= 1.75164 train_acc= 0.79167 val_loss= 1.77997 val_acc= 0.50400 time= 0.02853\n",
      "Epoch: 0009 train_loss= 1.74751 train_acc= 0.77500 val_loss= 1.77841 val_acc= 0.51400 time= 0.03050\n",
      "Epoch: 0010 train_loss= 1.74177 train_acc= 0.83333 val_loss= 1.77692 val_acc= 0.52000 time= 0.03085\n",
      "Epoch: 0011 train_loss= 1.73514 train_acc= 0.80833 val_loss= 1.77545 val_acc= 0.54200 time= 0.02640\n",
      "Epoch: 0012 train_loss= 1.72661 train_acc= 0.85000 val_loss= 1.77408 val_acc= 0.55200 time= 0.02667\n",
      "Epoch: 0013 train_loss= 1.71158 train_acc= 0.85000 val_loss= 1.77276 val_acc= 0.56000 time= 0.02698\n",
      "Epoch: 0014 train_loss= 1.70929 train_acc= 0.85833 val_loss= 1.77148 val_acc= 0.56200 time= 0.02666\n",
      "Epoch: 0015 train_loss= 1.69715 train_acc= 0.89167 val_loss= 1.77023 val_acc= 0.56400 time= 0.02684\n",
      "Epoch: 0016 train_loss= 1.69234 train_acc= 0.87500 val_loss= 1.76897 val_acc= 0.56800 time= 0.02653\n",
      "Epoch: 0017 train_loss= 1.68609 train_acc= 0.90000 val_loss= 1.76769 val_acc= 0.57200 time= 0.02943\n",
      "Epoch: 0018 train_loss= 1.67787 train_acc= 0.85833 val_loss= 1.76634 val_acc= 0.57000 time= 0.02642\n",
      "Epoch: 0019 train_loss= 1.67432 train_acc= 0.92500 val_loss= 1.76494 val_acc= 0.57200 time= 0.02704\n",
      "Epoch: 0020 train_loss= 1.66147 train_acc= 0.89167 val_loss= 1.76347 val_acc= 0.57200 time= 0.02646\n",
      "Epoch: 0021 train_loss= 1.65819 train_acc= 0.81667 val_loss= 1.76186 val_acc= 0.57800 time= 0.02730\n",
      "Epoch: 0022 train_loss= 1.64356 train_acc= 0.84167 val_loss= 1.76018 val_acc= 0.58200 time= 0.02665\n",
      "Epoch: 0023 train_loss= 1.63531 train_acc= 0.88333 val_loss= 1.75849 val_acc= 0.58400 time= 0.02674\n",
      "Epoch: 0024 train_loss= 1.63347 train_acc= 0.90833 val_loss= 1.75671 val_acc= 0.58200 time= 0.02612\n",
      "Epoch: 0025 train_loss= 1.61753 train_acc= 0.86667 val_loss= 1.75482 val_acc= 0.58400 time= 0.02988\n",
      "Epoch: 0026 train_loss= 1.61745 train_acc= 0.87500 val_loss= 1.75289 val_acc= 0.59000 time= 0.02755\n",
      "Epoch: 0027 train_loss= 1.60859 train_acc= 0.88333 val_loss= 1.75082 val_acc= 0.59400 time= 0.02690\n",
      "Epoch: 0028 train_loss= 1.58192 train_acc= 0.89167 val_loss= 1.74875 val_acc= 0.60000 time= 0.02648\n",
      "Epoch: 0029 train_loss= 1.60037 train_acc= 0.88333 val_loss= 1.74653 val_acc= 0.60200 time= 0.02729\n",
      "Epoch: 0030 train_loss= 1.57044 train_acc= 0.86667 val_loss= 1.74430 val_acc= 0.60600 time= 0.02711\n",
      "Epoch: 0031 train_loss= 1.56010 train_acc= 0.85000 val_loss= 1.74189 val_acc= 0.60800 time= 0.02663\n",
      "Epoch: 0032 train_loss= 1.54295 train_acc= 0.88333 val_loss= 1.73942 val_acc= 0.60800 time= 0.02703\n",
      "Epoch: 0033 train_loss= 1.53230 train_acc= 0.90000 val_loss= 1.73690 val_acc= 0.60800 time= 0.02962\n",
      "Epoch: 0034 train_loss= 1.53375 train_acc= 0.92500 val_loss= 1.73435 val_acc= 0.61400 time= 0.02726\n",
      "Epoch: 0035 train_loss= 1.50363 train_acc= 0.92500 val_loss= 1.73179 val_acc= 0.61800 time= 0.02664\n",
      "Epoch: 0036 train_loss= 1.49551 train_acc= 0.90000 val_loss= 1.72905 val_acc= 0.62600 time= 0.02674\n",
      "Epoch: 0037 train_loss= 1.53152 train_acc= 0.81667 val_loss= 1.72636 val_acc= 0.62800 time= 0.02644\n",
      "Epoch: 0038 train_loss= 1.49809 train_acc= 0.86667 val_loss= 1.72363 val_acc= 0.63200 time= 0.02850\n",
      "Epoch: 0039 train_loss= 1.47554 train_acc= 0.91667 val_loss= 1.72081 val_acc= 0.63200 time= 0.02627\n",
      "Epoch: 0040 train_loss= 1.45025 train_acc= 0.89167 val_loss= 1.71799 val_acc= 0.63000 time= 0.02782\n",
      "Epoch: 0041 train_loss= 1.46086 train_acc= 0.84167 val_loss= 1.71496 val_acc= 0.63000 time= 0.03049\n",
      "Epoch: 0042 train_loss= 1.45733 train_acc= 0.90000 val_loss= 1.71193 val_acc= 0.63000 time= 0.02689\n",
      "Epoch: 0043 train_loss= 1.41931 train_acc= 0.87500 val_loss= 1.70891 val_acc= 0.63800 time= 0.02605\n",
      "Epoch: 0044 train_loss= 1.43423 train_acc= 0.87500 val_loss= 1.70581 val_acc= 0.65400 time= 0.02741\n",
      "Epoch: 0045 train_loss= 1.43880 train_acc= 0.87500 val_loss= 1.70260 val_acc= 0.65600 time= 0.02691\n",
      "Epoch: 0046 train_loss= 1.39698 train_acc= 0.90000 val_loss= 1.69912 val_acc= 0.65800 time= 0.03026\n",
      "Epoch: 0047 train_loss= 1.40382 train_acc= 0.89167 val_loss= 1.69557 val_acc= 0.67000 time= 0.03084\n",
      "Epoch: 0048 train_loss= 1.37889 train_acc= 0.88333 val_loss= 1.69211 val_acc= 0.68200 time= 0.02994\n",
      "Epoch: 0049 train_loss= 1.36262 train_acc= 0.87500 val_loss= 1.68861 val_acc= 0.68600 time= 0.03253\n",
      "Epoch: 0050 train_loss= 1.35439 train_acc= 0.90833 val_loss= 1.68516 val_acc= 0.68600 time= 0.03042\n",
      "Epoch: 0051 train_loss= 1.34431 train_acc= 0.90833 val_loss= 1.68156 val_acc= 0.68400 time= 0.03719\n",
      "Epoch: 0052 train_loss= 1.33265 train_acc= 0.89167 val_loss= 1.67784 val_acc= 0.68400 time= 0.02880\n",
      "Epoch: 0053 train_loss= 1.35381 train_acc= 0.88333 val_loss= 1.67416 val_acc= 0.68800 time= 0.02674\n",
      "Epoch: 0054 train_loss= 1.28668 train_acc= 0.90833 val_loss= 1.67048 val_acc= 0.68800 time= 0.02665\n",
      "Epoch: 0055 train_loss= 1.32591 train_acc= 0.91667 val_loss= 1.66678 val_acc= 0.68600 time= 0.02623\n",
      "Epoch: 0056 train_loss= 1.28664 train_acc= 0.90833 val_loss= 1.66311 val_acc= 0.68200 time= 0.02756\n",
      "Epoch: 0057 train_loss= 1.28888 train_acc= 0.91667 val_loss= 1.65933 val_acc= 0.68000 time= 0.03259\n",
      "Epoch: 0058 train_loss= 1.26753 train_acc= 0.90000 val_loss= 1.65535 val_acc= 0.68400 time= 0.02716\n",
      "Epoch: 0059 train_loss= 1.28011 train_acc= 0.92500 val_loss= 1.65129 val_acc= 0.68600 time= 0.02643\n",
      "Epoch: 0060 train_loss= 1.24574 train_acc= 0.93333 val_loss= 1.64730 val_acc= 0.68600 time= 0.02706\n",
      "Epoch: 0061 train_loss= 1.24723 train_acc= 0.92500 val_loss= 1.64314 val_acc= 0.68400 time= 0.02704\n",
      "Epoch: 0062 train_loss= 1.28352 train_acc= 0.85000 val_loss= 1.63897 val_acc= 0.68600 time= 0.02570\n",
      "Epoch: 0063 train_loss= 1.25478 train_acc= 0.90833 val_loss= 1.63494 val_acc= 0.69000 time= 0.02582\n",
      "Epoch: 0064 train_loss= 1.21853 train_acc= 0.90833 val_loss= 1.63079 val_acc= 0.69200 time= 0.02991\n",
      "Epoch: 0065 train_loss= 1.20923 train_acc= 0.93333 val_loss= 1.62672 val_acc= 0.69200 time= 0.02705\n",
      "Epoch: 0066 train_loss= 1.25296 train_acc= 0.95000 val_loss= 1.62265 val_acc= 0.69800 time= 0.02609\n",
      "Epoch: 0067 train_loss= 1.22444 train_acc= 0.92500 val_loss= 1.61855 val_acc= 0.70400 time= 0.02566\n",
      "Epoch: 0068 train_loss= 1.18121 train_acc= 0.91667 val_loss= 1.61434 val_acc= 0.70400 time= 0.02760\n",
      "Epoch: 0069 train_loss= 1.18432 train_acc= 0.91667 val_loss= 1.61014 val_acc= 0.70400 time= 0.03799\n",
      "Epoch: 0070 train_loss= 1.13963 train_acc= 0.93333 val_loss= 1.60602 val_acc= 0.70400 time= 0.03143\n",
      "Epoch: 0071 train_loss= 1.19986 train_acc= 0.93333 val_loss= 1.60211 val_acc= 0.70600 time= 0.03327\n",
      "Epoch: 0072 train_loss= 1.15603 train_acc= 0.92500 val_loss= 1.59844 val_acc= 0.70800 time= 0.03610\n",
      "Epoch: 0073 train_loss= 1.13508 train_acc= 0.90833 val_loss= 1.59469 val_acc= 0.71800 time= 0.03047\n",
      "Epoch: 0074 train_loss= 1.12983 train_acc= 0.91667 val_loss= 1.59074 val_acc= 0.71600 time= 0.03545\n",
      "Epoch: 0075 train_loss= 1.10837 train_acc= 0.93333 val_loss= 1.58665 val_acc= 0.71600 time= 0.02941\n",
      "Epoch: 0076 train_loss= 1.10599 train_acc= 0.91667 val_loss= 1.58260 val_acc= 0.71600 time= 0.02746\n",
      "Epoch: 0077 train_loss= 1.11965 train_acc= 0.90000 val_loss= 1.57859 val_acc= 0.71600 time= 0.02590\n",
      "Epoch: 0078 train_loss= 1.08848 train_acc= 0.90000 val_loss= 1.57436 val_acc= 0.71400 time= 0.02795\n",
      "Epoch: 0079 train_loss= 1.12718 train_acc= 0.95833 val_loss= 1.57035 val_acc= 0.71400 time= 0.02889\n",
      "Epoch: 0080 train_loss= 1.08727 train_acc= 0.92500 val_loss= 1.56649 val_acc= 0.71600 time= 0.02772\n",
      "Epoch: 0081 train_loss= 1.07719 train_acc= 0.94167 val_loss= 1.56244 val_acc= 0.71600 time= 0.02656\n",
      "Epoch: 0082 train_loss= 1.06598 train_acc= 0.95000 val_loss= 1.55864 val_acc= 0.71200 time= 0.02676\n",
      "Epoch: 0083 train_loss= 1.06462 train_acc= 0.94167 val_loss= 1.55497 val_acc= 0.71800 time= 0.02699\n",
      "Epoch: 0084 train_loss= 1.08893 train_acc= 0.90000 val_loss= 1.55129 val_acc= 0.71600 time= 0.02713\n",
      "Epoch: 0085 train_loss= 1.03009 train_acc= 0.93333 val_loss= 1.54768 val_acc= 0.71600 time= 0.02738\n",
      "Epoch: 0086 train_loss= 1.04069 train_acc= 0.92500 val_loss= 1.54403 val_acc= 0.71400 time= 0.02980\n",
      "Epoch: 0087 train_loss= 1.04279 train_acc= 0.91667 val_loss= 1.54056 val_acc= 0.71400 time= 0.03792\n",
      "Epoch: 0088 train_loss= 1.02326 train_acc= 0.90000 val_loss= 1.53706 val_acc= 0.71400 time= 0.03245\n",
      "Epoch: 0089 train_loss= 1.03376 train_acc= 0.92500 val_loss= 1.53360 val_acc= 0.71400 time= 0.03857\n",
      "Epoch: 0090 train_loss= 1.02271 train_acc= 0.92500 val_loss= 1.52997 val_acc= 0.71600 time= 0.03192\n",
      "Epoch: 0091 train_loss= 1.04321 train_acc= 0.92500 val_loss= 1.52620 val_acc= 0.71400 time= 0.03050\n",
      "Epoch: 0092 train_loss= 1.00157 train_acc= 0.95833 val_loss= 1.52257 val_acc= 0.71400 time= 0.03346\n",
      "Epoch: 0093 train_loss= 0.96063 train_acc= 0.94167 val_loss= 1.51888 val_acc= 0.71200 time= 0.03227\n",
      "Epoch: 0094 train_loss= 1.00491 train_acc= 0.91667 val_loss= 1.51513 val_acc= 0.71400 time= 0.02796\n",
      "Epoch: 0095 train_loss= 0.99602 train_acc= 0.92500 val_loss= 1.51156 val_acc= 0.71400 time= 0.02691\n",
      "Epoch: 0096 train_loss= 1.01251 train_acc= 0.92500 val_loss= 1.50799 val_acc= 0.71200 time= 0.02585\n",
      "Epoch: 0097 train_loss= 0.99937 train_acc= 0.90833 val_loss= 1.50448 val_acc= 0.71200 time= 0.02586\n",
      "Epoch: 0098 train_loss= 1.00815 train_acc= 0.95000 val_loss= 1.50101 val_acc= 0.71200 time= 0.02599\n",
      "Epoch: 0099 train_loss= 0.97744 train_acc= 0.92500 val_loss= 1.49759 val_acc= 0.71200 time= 0.02684\n",
      "Epoch: 0100 train_loss= 0.95587 train_acc= 0.90833 val_loss= 1.49431 val_acc= 0.71200 time= 0.02859\n",
      "Epoch: 0101 train_loss= 0.95599 train_acc= 0.92500 val_loss= 1.49118 val_acc= 0.71400 time= 0.03012\n",
      "Epoch: 0102 train_loss= 0.96347 train_acc= 0.95000 val_loss= 1.48874 val_acc= 0.71400 time= 0.03785\n",
      "Epoch: 0103 train_loss= 0.97606 train_acc= 0.91667 val_loss= 1.48617 val_acc= 0.71400 time= 0.03317\n",
      "Epoch: 0104 train_loss= 0.95049 train_acc= 0.93333 val_loss= 1.48334 val_acc= 0.71600 time= 0.03425\n",
      "Epoch: 0105 train_loss= 0.93458 train_acc= 0.93333 val_loss= 1.48058 val_acc= 0.71600 time= 0.03395\n",
      "Epoch: 0106 train_loss= 0.93535 train_acc= 0.94167 val_loss= 1.47791 val_acc= 0.71400 time= 0.03005\n",
      "Epoch: 0107 train_loss= 0.92051 train_acc= 0.92500 val_loss= 1.47524 val_acc= 0.71400 time= 0.03523\n",
      "Epoch: 0108 train_loss= 0.96012 train_acc= 0.91667 val_loss= 1.47238 val_acc= 0.71200 time= 0.02865\n",
      "Epoch: 0109 train_loss= 0.92236 train_acc= 0.95000 val_loss= 1.46977 val_acc= 0.71400 time= 0.02663\n",
      "Epoch: 0110 train_loss= 0.89794 train_acc= 0.92500 val_loss= 1.46701 val_acc= 0.71400 time= 0.02743\n",
      "Epoch: 0111 train_loss= 0.92026 train_acc= 0.94167 val_loss= 1.46392 val_acc= 0.71600 time= 0.02608\n",
      "Epoch: 0112 train_loss= 0.90861 train_acc= 0.94167 val_loss= 1.46086 val_acc= 0.71400 time= 0.02620\n",
      "Epoch: 0113 train_loss= 0.89554 train_acc= 0.94167 val_loss= 1.45717 val_acc= 0.71400 time= 0.02567\n",
      "Epoch: 0114 train_loss= 0.96613 train_acc= 0.89167 val_loss= 1.45342 val_acc= 0.71600 time= 0.02847\n",
      "Epoch: 0115 train_loss= 0.91164 train_acc= 0.95000 val_loss= 1.44999 val_acc= 0.71400 time= 0.02917\n",
      "Epoch: 0116 train_loss= 0.91297 train_acc= 0.93333 val_loss= 1.44652 val_acc= 0.71400 time= 0.02707\n",
      "Epoch: 0117 train_loss= 0.89151 train_acc= 0.95000 val_loss= 1.44302 val_acc= 0.71200 time= 0.02717\n",
      "Epoch: 0118 train_loss= 0.92807 train_acc= 0.95000 val_loss= 1.43966 val_acc= 0.71600 time= 0.03273\n",
      "Epoch: 0119 train_loss= 0.85832 train_acc= 0.92500 val_loss= 1.43684 val_acc= 0.71400 time= 0.03495\n",
      "Epoch: 0120 train_loss= 0.91677 train_acc= 0.95000 val_loss= 1.43406 val_acc= 0.71600 time= 0.03077\n",
      "Epoch: 0121 train_loss= 0.95636 train_acc= 0.89167 val_loss= 1.43147 val_acc= 0.71600 time= 0.03113\n",
      "Epoch: 0122 train_loss= 0.87725 train_acc= 0.90833 val_loss= 1.42916 val_acc= 0.71800 time= 0.02899\n",
      "Epoch: 0123 train_loss= 0.87882 train_acc= 0.96667 val_loss= 1.42700 val_acc= 0.72000 time= 0.02979\n",
      "Epoch: 0124 train_loss= 0.86785 train_acc= 0.93333 val_loss= 1.42516 val_acc= 0.72000 time= 0.03379\n",
      "Epoch: 0125 train_loss= 0.87024 train_acc= 0.92500 val_loss= 1.42330 val_acc= 0.72000 time= 0.03184\n",
      "Epoch: 0126 train_loss= 0.94655 train_acc= 0.90833 val_loss= 1.42149 val_acc= 0.71800 time= 0.02663\n",
      "Epoch: 0127 train_loss= 0.89074 train_acc= 0.92500 val_loss= 1.41910 val_acc= 0.72000 time= 0.02651\n",
      "Epoch: 0128 train_loss= 0.88241 train_acc= 0.92500 val_loss= 1.41689 val_acc= 0.72200 time= 0.02656\n",
      "Epoch: 0129 train_loss= 0.87119 train_acc= 0.94167 val_loss= 1.41439 val_acc= 0.72000 time= 0.02867\n",
      "Epoch: 0130 train_loss= 0.84917 train_acc= 0.93333 val_loss= 1.41189 val_acc= 0.71800 time= 0.02698\n",
      "Epoch: 0131 train_loss= 0.86894 train_acc= 0.93333 val_loss= 1.40934 val_acc= 0.72000 time= 0.02627\n",
      "Epoch: 0132 train_loss= 0.87891 train_acc= 0.93333 val_loss= 1.40729 val_acc= 0.72000 time= 0.02600\n",
      "Epoch: 0133 train_loss= 0.86065 train_acc= 0.94167 val_loss= 1.40510 val_acc= 0.71600 time= 0.02714\n",
      "Epoch: 0134 train_loss= 0.86063 train_acc= 0.93333 val_loss= 1.40286 val_acc= 0.71600 time= 0.03056\n",
      "Epoch: 0135 train_loss= 0.85404 train_acc= 0.91667 val_loss= 1.40057 val_acc= 0.71400 time= 0.03429\n",
      "Epoch: 0136 train_loss= 0.83276 train_acc= 0.95000 val_loss= 1.39827 val_acc= 0.71400 time= 0.03078\n",
      "Epoch: 0137 train_loss= 0.83559 train_acc= 0.94167 val_loss= 1.39619 val_acc= 0.71400 time= 0.03097\n",
      "Epoch: 0138 train_loss= 0.83892 train_acc= 0.93333 val_loss= 1.39389 val_acc= 0.71600 time= 0.02828\n",
      "Epoch: 0139 train_loss= 0.83649 train_acc= 0.95000 val_loss= 1.39180 val_acc= 0.71600 time= 0.03773\n",
      "Epoch: 0140 train_loss= 0.82781 train_acc= 0.95000 val_loss= 1.39002 val_acc= 0.71600 time= 0.03061\n",
      "Epoch: 0141 train_loss= 0.84473 train_acc= 0.95000 val_loss= 1.38837 val_acc= 0.71600 time= 0.02866\n",
      "Epoch: 0142 train_loss= 0.81023 train_acc= 0.94167 val_loss= 1.38650 val_acc= 0.71600 time= 0.02705\n",
      "Epoch: 0143 train_loss= 0.82584 train_acc= 0.93333 val_loss= 1.38454 val_acc= 0.72200 time= 0.02640\n",
      "Epoch: 0144 train_loss= 0.83906 train_acc= 0.90000 val_loss= 1.38325 val_acc= 0.72200 time= 0.02686\n",
      "Epoch: 0145 train_loss= 0.77757 train_acc= 0.96667 val_loss= 1.38190 val_acc= 0.72200 time= 0.02691\n",
      "Epoch: 0146 train_loss= 0.85302 train_acc= 0.94167 val_loss= 1.38094 val_acc= 0.71600 time= 0.02607\n",
      "Epoch: 0147 train_loss= 0.83531 train_acc= 0.95000 val_loss= 1.38021 val_acc= 0.71800 time= 0.02748\n",
      "Epoch: 0148 train_loss= 0.82534 train_acc= 0.91667 val_loss= 1.37944 val_acc= 0.71800 time= 0.02588\n",
      "Epoch: 0149 train_loss= 0.80191 train_acc= 0.95000 val_loss= 1.37838 val_acc= 0.71600 time= 0.02709\n",
      "Epoch: 0150 train_loss= 0.79236 train_acc= 0.95000 val_loss= 1.37749 val_acc= 0.71400 time= 0.02999\n",
      "Epoch: 0151 train_loss= 0.78391 train_acc= 0.94167 val_loss= 1.37671 val_acc= 0.71200 time= 0.02610\n",
      "Epoch: 0152 train_loss= 0.79014 train_acc= 0.92500 val_loss= 1.37544 val_acc= 0.71200 time= 0.02689\n",
      "Epoch: 0153 train_loss= 0.88042 train_acc= 0.90833 val_loss= 1.37426 val_acc= 0.71000 time= 0.02680\n",
      "Epoch: 0154 train_loss= 0.82052 train_acc= 0.94167 val_loss= 1.37255 val_acc= 0.71000 time= 0.02587\n",
      "Epoch: 0155 train_loss= 0.73510 train_acc= 0.94167 val_loss= 1.37122 val_acc= 0.71000 time= 0.02525\n",
      "Epoch: 0156 train_loss= 0.80740 train_acc= 0.92500 val_loss= 1.36990 val_acc= 0.71000 time= 0.02590\n",
      "Epoch: 0157 train_loss= 0.77367 train_acc= 0.95000 val_loss= 1.36838 val_acc= 0.71000 time= 0.02688\n",
      "Epoch: 0158 train_loss= 0.77762 train_acc= 0.93333 val_loss= 1.36657 val_acc= 0.71200 time= 0.03699\n",
      "Epoch: 0159 train_loss= 0.76038 train_acc= 0.96667 val_loss= 1.36411 val_acc= 0.71600 time= 0.03315\n",
      "Epoch: 0160 train_loss= 0.77627 train_acc= 0.95000 val_loss= 1.36123 val_acc= 0.71800 time= 0.03320\n",
      "Epoch: 0161 train_loss= 0.78251 train_acc= 0.96667 val_loss= 1.35871 val_acc= 0.71800 time= 0.03074\n",
      "Epoch: 0162 train_loss= 0.77747 train_acc= 0.95833 val_loss= 1.35590 val_acc= 0.71600 time= 0.03157\n",
      "Epoch: 0163 train_loss= 0.81674 train_acc= 0.91667 val_loss= 1.35370 val_acc= 0.71600 time= 0.03330\n",
      "Epoch: 0164 train_loss= 0.77657 train_acc= 0.95000 val_loss= 1.35198 val_acc= 0.71800 time= 0.03481\n",
      "Epoch: 0165 train_loss= 0.78776 train_acc= 0.95000 val_loss= 1.35091 val_acc= 0.71800 time= 0.02869\n",
      "Epoch: 0166 train_loss= 0.73126 train_acc= 0.97500 val_loss= 1.34995 val_acc= 0.71800 time= 0.02563\n",
      "Epoch: 0167 train_loss= 0.76484 train_acc= 0.95000 val_loss= 1.34907 val_acc= 0.71600 time= 0.02684\n",
      "Epoch: 0168 train_loss= 0.77105 train_acc= 0.94167 val_loss= 1.34826 val_acc= 0.71600 time= 0.02690\n",
      "Epoch: 0169 train_loss= 0.75029 train_acc= 0.95833 val_loss= 1.34712 val_acc= 0.71600 time= 0.02569\n",
      "Epoch: 0170 train_loss= 0.78167 train_acc= 0.96667 val_loss= 1.34599 val_acc= 0.71200 time= 0.02577\n",
      "Epoch: 0171 train_loss= 0.71746 train_acc= 0.97500 val_loss= 1.34461 val_acc= 0.71400 time= 0.03480\n",
      "Epoch: 0172 train_loss= 0.74186 train_acc= 0.94167 val_loss= 1.34341 val_acc= 0.71400 time= 0.03169\n",
      "Epoch: 0173 train_loss= 0.70410 train_acc= 0.98333 val_loss= 1.34228 val_acc= 0.71600 time= 0.03020\n",
      "Epoch: 0174 train_loss= 0.74131 train_acc= 0.96667 val_loss= 1.34146 val_acc= 0.71600 time= 0.03266\n",
      "Epoch: 0175 train_loss= 0.70293 train_acc= 0.96667 val_loss= 1.34060 val_acc= 0.71200 time= 0.03065\n",
      "Epoch: 0176 train_loss= 0.75502 train_acc= 0.93333 val_loss= 1.34019 val_acc= 0.71200 time= 0.03860\n",
      "Epoch: 0177 train_loss= 0.78417 train_acc= 0.94167 val_loss= 1.33951 val_acc= 0.71200 time= 0.03084\n",
      "Epoch: 0178 train_loss= 0.71897 train_acc= 0.96667 val_loss= 1.33852 val_acc= 0.71600 time= 0.03634\n",
      "Epoch: 0179 train_loss= 0.68623 train_acc= 0.96667 val_loss= 1.33764 val_acc= 0.71400 time= 0.03058\n",
      "Epoch: 0180 train_loss= 0.71435 train_acc= 0.95833 val_loss= 1.33643 val_acc= 0.71200 time= 0.02811\n",
      "Epoch: 0181 train_loss= 0.70705 train_acc= 0.95000 val_loss= 1.33541 val_acc= 0.71200 time= 0.02638\n",
      "Epoch: 0182 train_loss= 0.68500 train_acc= 0.99167 val_loss= 1.33433 val_acc= 0.71200 time= 0.02557\n",
      "Epoch: 0183 train_loss= 0.70434 train_acc= 0.96667 val_loss= 1.33259 val_acc= 0.70800 time= 0.02660\n",
      "Epoch: 0184 train_loss= 0.70969 train_acc= 0.91667 val_loss= 1.33133 val_acc= 0.71000 time= 0.02688\n",
      "Epoch: 0185 train_loss= 0.70096 train_acc= 0.98333 val_loss= 1.32950 val_acc= 0.71000 time= 0.02705\n",
      "Epoch: 0186 train_loss= 0.71238 train_acc= 0.95000 val_loss= 1.32750 val_acc= 0.71400 time= 0.02630\n",
      "Epoch: 0187 train_loss= 0.70574 train_acc= 0.93333 val_loss= 1.32557 val_acc= 0.71400 time= 0.03084\n",
      "Epoch: 0188 train_loss= 0.72253 train_acc= 0.95000 val_loss= 1.32355 val_acc= 0.71000 time= 0.02545\n",
      "Epoch: 0189 train_loss= 0.69762 train_acc= 0.95833 val_loss= 1.32145 val_acc= 0.71200 time= 0.02547\n",
      "Epoch: 0190 train_loss= 0.73765 train_acc= 0.95000 val_loss= 1.31944 val_acc= 0.71200 time= 0.02580\n",
      "Epoch: 0191 train_loss= 0.70410 train_acc= 0.95833 val_loss= 1.31717 val_acc= 0.71200 time= 0.02622\n",
      "Epoch: 0192 train_loss= 0.69184 train_acc= 0.95000 val_loss= 1.31493 val_acc= 0.71400 time= 0.02676\n",
      "Epoch: 0193 train_loss= 0.69906 train_acc= 0.96667 val_loss= 1.31264 val_acc= 0.71400 time= 0.02599\n",
      "Epoch: 0194 train_loss= 0.74271 train_acc= 0.95000 val_loss= 1.31031 val_acc= 0.71600 time= 0.02623\n",
      "Epoch: 0195 train_loss= 0.74186 train_acc= 0.93333 val_loss= 1.30792 val_acc= 0.71600 time= 0.02715\n",
      "Epoch: 0196 train_loss= 0.68627 train_acc= 0.94167 val_loss= 1.30580 val_acc= 0.71400 time= 0.02669\n",
      "Epoch: 0197 train_loss= 0.68511 train_acc= 0.95000 val_loss= 1.30349 val_acc= 0.71200 time= 0.02568\n",
      "Epoch: 0198 train_loss= 0.69452 train_acc= 0.95833 val_loss= 1.30215 val_acc= 0.71200 time= 0.02664\n",
      "Epoch: 0199 train_loss= 0.69583 train_acc= 0.95833 val_loss= 1.30073 val_acc= 0.71200 time= 0.02639\n",
      "Epoch: 0200 train_loss= 0.66276 train_acc= 0.96667 val_loss= 1.29952 val_acc= 0.71200 time= 0.02604\n",
      "Optimization Finished!\n",
      "Test set results: cost= 1.28585 accuracy= 0.72000 time= 0.01335\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "epochs = 200\n",
    "dropout = 0.5\n",
    "early_stopping = 10\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    t = time.time()\n",
    "    # Construct feed dictionary\n",
    "    feed_dict = construct_feed_dict(features, support, y_train, train_mask, placeholders)\n",
    "    feed_dict.update({placeholders['dropout']:dropout})\n",
    "\n",
    "    # Training step\n",
    "    outs = sess.run([model.opt_op, model.loss, model.accuracy], feed_dict=feed_dict)\n",
    "\n",
    "    # Validation\n",
    "    cost, acc, duration = evaluate(features, support, y_val, val_mask, placeholders)\n",
    "    cost_val.append(cost)\n",
    "\n",
    "    # Print results\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(outs[1]),\n",
    "          \"train_acc=\", \"{:.5f}\".format(outs[2]), \"val_loss=\", \"{:.5f}\".format(cost),\n",
    "          \"val_acc=\", \"{:.5f}\".format(acc), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "    if epoch > early_stopping and cost_val[-1] > np.mean(cost_val[-(early_stopping+1):-1]):\n",
    "        print(\"Early stopping...\")\n",
    "        break\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "# Testing\n",
    "test_cost, test_acc, test_duration = evaluate(features, support, y_test, test_mask, placeholders)\n",
    "print(\"Test set results:\", \"cost=\", \"{:.5f}\".format(test_cost),\n",
    "      \"accuracy=\", \"{:.5f}\".format(test_acc), \"time=\", \"{:.5f}\".format(test_duration))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weights_0': <tf.Variable 'gcn/graphconvolution_1_vars/weights_0:0' shape=(3703, 16) dtype=float32_ref>}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'gcn/graphconvolution_2_vars/weights_0:0' shape=(16, 6) dtype=float32_ref>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1].vars.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.2140489   1.6216251  -1.9067668  -1.8295588  -2.0481884   2.3971107 ]\n",
      " [ 1.4534065   1.9322166  -1.465002    2.1159115  -1.8303217  -2.0061352 ]\n",
      " [-2.4929097   2.4627924  -2.0897002  -2.0380554   1.8251437   1.9101021 ]\n",
      " [ 1.9273038  -2.487797   -2.4137433   1.6289151  -2.4051871   2.0461771 ]\n",
      " [-1.8334296  -1.5462577   2.1881366   2.1788633  -1.7635618  -2.11568   ]\n",
      " [ 2.0998175   1.4780444  -1.9124211  -2.0544236   2.1584485  -1.972863  ]\n",
      " [-2.101468   -1.9553187  -2.1900764   2.1582403   1.6812649   1.7475207 ]\n",
      " [-2.2944875   2.4202244  -2.1072235  -1.7602531   2.0247805   1.1951679 ]\n",
      " [ 1.0059295   1.7078187  -0.0147922   1.445493   -2.1847682  -2.296212  ]\n",
      " [-2.4198601   2.076082    2.0305727   1.669824   -2.2009742  -1.7315618 ]\n",
      " [-1.6457802  -1.693406    2.4736502  -2.060135    2.1198807   1.4583666 ]\n",
      " [-1.490863   -2.0959952   2.1890795  -1.8859533   1.7571442   1.6943876 ]\n",
      " [-0.94960743 -2.0994654   2.3312528  -2.3186522  -2.2903838   1.9181403 ]\n",
      " [-1.7107073  -2.082297    2.2953272  -1.9463959   2.0367062   0.9587923 ]\n",
      " [ 2.134835   -2.2981236  -1.9559063   1.7013601   2.057274   -2.0765321 ]\n",
      " [ 2.2913418   1.8314946   0.08575663 -2.0320308  -1.9907998  -2.0111632 ]]\n"
     ]
    }
   ],
   "source": [
    "print((sess.run(model.layers[1].vars.values()[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Can we do a clustering on the last part? If so, what happens? "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
